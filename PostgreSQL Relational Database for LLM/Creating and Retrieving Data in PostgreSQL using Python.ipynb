{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a4bab0-c707-4806-8e1c-9eb8272e5d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3545920667.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexport TF_ENABLE_ONEDNN_OPTS=0\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185ea17c-7294-4d21-b772-fcc159b63f18",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3011202380.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexport TF_CPP_MIN_LOG_LEVEL=2\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "export TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8f436e-9077-47a2-9129-5e36df1e8690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ekadw\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ekadw\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ekadw\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "https://tfhub.dev/google/universal-sentence-encoder/4 does not appear to be a valid module.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001b[39m, in \u001b[36mDownloadManager.download_and_uncompress\u001b[39m\u001b[34m(self, fileobj, dst_path)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m   file_utils.extract_tarfile_to_destination(\n\u001b[32m    193\u001b[39m       fileobj, dst_path, log_function=\u001b[38;5;28mself\u001b[39m._log_progress)\n\u001b[32m    194\u001b[39m   total_size_str = tf_utils.bytes_to_readable_str(\n\u001b[32m    195\u001b[39m       \u001b[38;5;28mself\u001b[39m._total_bytes_downloaded, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:52\u001b[39m, in \u001b[36mextract_tarfile_to_destination\u001b[39m\u001b[34m(fileobj, dst_path, log_function)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isfile():\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m   extract_file(tgz, tarinfo, abs_target_path, log_function=log_function)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tarinfo.isdir():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\file_utils.py:35\u001b[39m, in \u001b[36mextract_file\u001b[39m\u001b[34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m   buf = src.read(buffer_size)\n\u001b[32m     36\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\tarfile.py:701\u001b[39m, in \u001b[36m_FileInFile.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     buf = \u001b[38;5;28mself\u001b[39m.read(\u001b[38;5;28mlen\u001b[39m(b))\n\u001b[32m    702\u001b[39m     b[:\u001b[38;5;28mlen\u001b[39m(buf)] = buf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\tarfile.py:692\u001b[39m, in \u001b[36m_FileInFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) != length:\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[33m\"\u001b[39m\u001b[33munexpected end of data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    693\u001b[39m buf += b\n",
      "\u001b[31mReadError\u001b[39m: unexpected end of data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpsycopg2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = hub.load(\u001b[33m\"\u001b[39m\u001b[33mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# PostgreSQL database configuration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(handle, tags, options)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     99\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % handle)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m module_path = resolve(handle)\n\u001b[32m    101\u001b[39m is_hub_module_v1 = tf.io.gfile.exists(_get_module_proto_path(module_path))\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001b[39m, in \u001b[36mresolve\u001b[39m\u001b[34m(handle)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve\u001b[39m(handle):\n\u001b[32m     32\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[33;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m \u001b[33;03m    A string representing the Module path.\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m registry.resolver(handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[39m, in \u001b[36mMultiImplRegister.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     48\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m impl(*args, **kwargs)\n\u001b[32m     50\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     51\u001b[39m     fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001b[39m, in \u001b[36mHttpCompressedFileResolver.__call__\u001b[39m\u001b[34m(self, handle)\u001b[39m\n\u001b[32m     77\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._call_urlopen(request)\n\u001b[32m     78\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[32m     79\u001b[39m       response, tmp_dir)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resolver.atomic_download(handle, download, module_dir,\n\u001b[32m     82\u001b[39m                                 \u001b[38;5;28mself\u001b[39m._lock_file_timeout_sec())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001b[39m, in \u001b[36matomic_download\u001b[39m\u001b[34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[39m\n\u001b[32m    419\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading TF-Hub Module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, handle)\n\u001b[32m    420\u001b[39m tf.compat.v1.gfile.MakeDirs(tmp_dir)\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m download_fn(handle, tmp_dir)\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# Write module descriptor to capture information about which module was\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# content.\u001b[39;00m\n\u001b[32m    431\u001b[39m _write_module_descriptor_file(handle, module_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001b[39m, in \u001b[36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[39m\u001b[34m(handle, tmp_dir)\u001b[39m\n\u001b[32m     75\u001b[39m request = urllib.request.Request(\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._append_compressed_format_query(handle))\n\u001b[32m     77\u001b[39m response = \u001b[38;5;28mself\u001b[39m._call_urlopen(request)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resolver.DownloadManager(handle).download_and_uncompress(\n\u001b[32m     79\u001b[39m     response, tmp_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\PostgreSQL\\Lib\\site-packages\\tensorflow_hub\\resolver.py:200\u001b[39m, in \u001b[36mDownloadManager.download_and_uncompress\u001b[39m\u001b[34m(self, fileobj, dst_path)\u001b[39m\n\u001b[32m    196\u001b[39m   \u001b[38;5;28mself\u001b[39m._print_download_progress_msg(\n\u001b[32m    197\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mDownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, Total size: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m._url, total_size_str),\n\u001b[32m    198\u001b[39m       flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m tarfile.ReadError:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m does not appear to be a valid module.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m._url)\n",
      "\u001b[31mOSError\u001b[39m: https://tfhub.dev/google/universal-sentence-encoder/4 does not appear to be a valid module."
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import tensorflow_hub as hub\n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# PostgreSQL database configuration\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'PASSWORD', # Replace the password with the password of your postgres instance\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "# Sample sentences\n",
    "sentences = ['Go to Try IBM watsonx.ai or Try watsonx.governance. If you sign up for watsonx.governance, you automatically provision watsonx.ai as well.',\n",
    "                                  'Yes, when you sign up for IBM watsonx.ai, you automatically provision the free version of the underlying services: Watson Studio, Watson Machine Learning, and IBM Cloud Object Storage. When you sign up for IBM watsonx.governance, you automatically provision the free version of Watson OpenScale and the free versions of the services for IBM watsonx.ai.',\n",
    "                                  'When you are ready to upgrade any of the underlying services for watsonx.ai or watsonx.governance, you can upgrade in place without losing any of your work or data. You must be the owner or administrator of the IBM Cloud account for a service to upgrade it. See Upgrading services on watsonx.'\n",
    "]\n",
    "\n",
    "\n",
    "def vectorize(sentence):\n",
    "    return model([sentence])[0].numpy().tolist()\n",
    "\n",
    "def add_data():\n",
    "    try:\n",
    "        # Connect to PostgreSQL database\n",
    "        connection = psycopg2.connect(**config)\n",
    "\n",
    "        # Create a cursor object\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Define SQL query to create table\n",
    "        create_table_query = '''\n",
    "            CREATE TABLE IF NOT EXISTS mysentences (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                sentence TEXT,\n",
    "                embedding INTEGER[]\n",
    "            );\n",
    "        '''\n",
    "\n",
    "        # Execute the query to create the table\n",
    "        cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "        print('Table created successfully')\n",
    "\n",
    "        # Insert data into the table\n",
    "        for sentence in sentences:\n",
    "            embeddings = vectorize(sentence)\n",
    "            insert_query = '''\n",
    "                INSERT INTO mysentences (sentence, embedding)\n",
    "                VALUES (%s, %s);\n",
    "            '''\n",
    "            cursor.execute(insert_query, (sentence, embeddings))\n",
    "            connection.commit()\n",
    "\n",
    "        # Fetch and display the data\n",
    "        select_query = \"SELECT * FROM mysentences;\"\n",
    "        cursor.execute(select_query)\n",
    "        rows = cursor.fetchall()\n",
    "        for row in rows:\n",
    "            print(row[1], end='\\n\\n')  # Print the 'sentence' column\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"PostgreSQL connection is closed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    add_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce8cec-7725-46d2-857c-434cc570a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# PostgreSQL configuration\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'PASSWORD', // Replace the password with the password of your postgres instance\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "conn = psycopg2.connect(**config)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def check_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_a = np.linalg.norm(v1)\n",
    "    norm_b = np.linalg.norm(v2)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "# Function to search for the closest match\n",
    "def search_a_match(model, query_str):\n",
    "    new_vec = model([query_str])[0]\n",
    "    cur.execute('SELECT * FROM mysentences')\n",
    "    rows = cur.fetchall()\n",
    "    match_dist = {}\n",
    "    for row in rows:\n",
    "        match_dist[row[1]] = check_similarity(row[2], new_vec)\n",
    "    sorted_matches = sorted(match_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_matches[0][0]\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load Universal Sentence Encoder model\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    print(\"Model loaded successfully\")\n",
    "    result = search_a_match(model, \"What does the kangaroo do?\")\n",
    "    print(f'The closest match is \"{result}\"')\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f50802-d475-43bc-9506-8cf62d28e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_you_know_facts = [\n",
    "    \"Eiffel Tower can be 15 centimeters taller during the summer. Due to thermal expansion, the iron structure expands when temperatures rise.\",\n",
    "    \"The Amazon Rainforest produces 20% of the world's oxygen.\",\n",
    "    \"Mount Everest grows by about a quarter of an inch (0.25 inches) each year.\",\n",
    "    \"There are more possible iterations of a game of chess than there are atoms in the known universe.\",\n",
    "    \"The Earth's oceans contain nearly 20 million tons of gold.\",\n",
    "    \"Canada has the longest coastline of any country in the world.\",\n",
    "    \"The city of Venice, Italy is built on 118 small islands.\",\n",
    "    \"The deepest part of the ocean, the Mariana Trench, reaches a depth of about 36,070 feet (10,994 meters).\",\n",
    "    \"The world's largest desert is Antarctica.\",\n",
    "    \"The human brain is more active during sleep than during the day when awake.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ee6dc-fa5b-4f09-9278-ed82c4b2d271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6124cf-fdd6-4237-b45f-c7e08f311193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-PostgreSQL]",
   "language": "python",
   "name": "conda-env-anaconda3-PostgreSQL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
